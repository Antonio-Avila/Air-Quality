---
title: "AQS"
author: "Antonio Avila"
date: "10/18/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(RAQSAPI)
library(keyring)
library(beepr)
library(usmap)
library(lubridate)
library(forecast)
library(plotly)
library(rjson)
library(forecast)
library(tseries)
library(quantmod)
library(maps)
```

## R Markdown

```{r enable AQS credentials, include = FALSE}
# key_set(service = 'AQSDatamart', username = 'aavila2802@gmail.com')

datamart_user <-  'aavila2802@gmail.com'
server <-  'AQSDatamart'
aqs_credentials(username = datamart_user, key_get(service = server, username = datamart_user))
```


```{r state codes, include = FALSE}

state_codes <- aqs_states()

```


```{r extract ozone data, include = FALSE}

# parameter 62104 is the max temp in a 24 Hr period
# parameter 62105 is the min temp in a 24 Hr period
# parameter 62106 is the temp diff in a 24 Hr period
# parameter 44201 is the ozone 
# parameter 42102 is carbon dioxide

# Working for the parameter code 45201 (benzine) but not for any other codes from the list above

# start_time = proc.time()
# ozone_texas <- aqs_sampledata_by_state(parameter = "44201",
#                                            bdate = as.Date("20200901",
#                                                            format="%Y%m%d"
#                                                            ),
#                                            edate = as.Date("20210901",
#                                                           format = "%Y%m%d"),
#                                            stateFIPS = "48"
#                                           )
# print("Time to Execute:")
# print(proc.time() - start_time)
# beep()

```


```{r import data and combine}

# ozone_texas_20_21 <- read_csv("ozone_texas_20_21.csv")
# ozone_texas_19_20 <- read_csv("ozone_texas_19_20.csv")
# ozone_texas_18_19 <- read_csv("ozone_texas_18_19.csv")
# ozone_texas_17_18 <- read_csv("ozone_texas_17_18.csv")
# 
# ozone_texas <- union(union(union(ozone_texas_17_18, ozone_texas_18_19), ozone_texas_19_20), ozone_texas_20_21)
# 
# write_csv(ozone_texas, "ozone_texas.csv")

ozone_texas <- read_csv("ozone_texas.csv")
ozone_texas["fips"] <- as.integer(ozone_texas$state_code) * 1000 + as.integer(ozone_texas$county_code)

```


```{r function for 8 hour ozone avg}


avg_8hr <- function(data){
  avgs = matrix(nrow = 24-8, ncol = 1)
  for(i in 1:(24-8)) { 
    avgs[i] = mean(data[i:(i+7)], na.rm = TRUE)
  }
  return(avgs)
}


max_8hr_avg <- function(data){
  max_avg = max(avg_8hr(data)) * 1000
  max_avg = trunc(max_avg)
  return(max_avg)
}


```


```{r verify with published data}
# Verified calculations with results from various days published by the Texas Commission on Environmental Quality
# 
# test_day <- ozone_texas %>% filter(date_local == "2019-10-02", site_number == "0416")
# test_day$sample_measurement %>% max_8hr_avg()

```



```{r find daily max avg per county}
daily_max_sites <- ozone_texas %>% 
  group_by(longitude, latitude, county, fips, site_number, date_local) %>% 
  summarize(max_8hr_avg = max_8hr_avg(sample_measurement)) %>% ungroup()

daily_max_counties <- daily_max_sites %>%
  group_by(county, fips, date_local) %>% 
  summarise(county_max = max(max_8hr_avg, na.rm = TRUE)) %>% ungroup()
daily_max_counties[is.infinite(daily_max_counties$county_max),]$county_max <- NA
```



```{r Harris county ozone lvl, include = FALSE}

# Normal gpplot, but working towards turning it into an interactive plot_ly plot 

# p <- daily_max_counties %>% filter(county == "Harris") %>% 
#   ggplot(aes(date_local, county_max)) + 
#     geom_line(aes(color = county_max)) + 
#     scale_color_gradient(low = "green", high = "red") +
#     geom_hline(yintercept = 70, color = "red") +
#     geom_smooth(method = "gam", se = FALSE) + 
#     ggtitle("Harris County Ozone levels 2017-2021") +
#     xlab("Date") +
#     ylab("Ozone 8-Hr Max (ppb)") +
#     annotate("text", x = as.Date("2018-01-01"), y = 71, label = "Unhealthy Level")
# p


p <- daily_max_counties %>% filter(county == "Harris") %>% 
  ggplot(aes(date_local, county_max)) + 
    geom_line(aes(color = "8-Hr Max Avg Ozone")) + 
    geom_hline(yintercept = 70, color = "red") +
    geom_smooth(method = "gam", se = FALSE, aes(color = "Local Model")) + 
    geom_smooth(method = "lm", aes(color = "Trend"), se = FALSE) +
    ggtitle( paste("Harris", "County Ozone levels 2017-2021")) +
    xlab("Date") +
    ylab("Ozone 8-Hr Max (ppb)") +
    annotate("text", x = as.Date("2018-01-01"), y = 71, label = "Unhealthy Level") +
    scale_color_manual(name = "Legend", values = c("#69b3a2", "#00A9FF", "purple"))
ggplotly(p, dynamicTicks = TRUE) %>%  layout(hovermode = "x") %>% rangeslider()

ozone_lm <- lm(county_max ~., data = harris_county)
summary(ozone_lm)


# Some statistics
adf.test(harris_county$county_max, k =365)
Box.test(harris_county$county_max, type = "Ljung", lag = 365)

par(mfcol = c(2,1))
Acf(harris_county$county_max, lag.max = 720)
Pacf(harris_county$county_max, lag.max = 720)
par(mfcol = c(1,1))
```



Even though there is some variability throughout the time period in the daily, maximum 8-hour ozone levels, the plot suggests there may be a slight downward trend. Fitting a linear model to the time series data rejects this notion. The p-value for the coefficient corresponding to the date yields a value of 0.47, meaning the coefficient and thus variable is not statistically significant. There is no evidence to suggest a statistically significant decrease in ozone levels with time. Granted, this model cannot be used for anything other than determining a trend since it is a very poor fit for the data. The data is obviously non-linear with some seasonality sprinkled in there, which makes sense since ozone levels should increase during summer periods. 


#----------------------  Mapping  ----------------------



```{r}
start_time <- Sys.time()


test_day <- daily_max_counties %>% filter(date_local == "2017-09-13")
county_df <- us_map("county") %>% filter(full == "Texas") %>% rename(state = full)
county_df$county <- str_replace(county_df$county, " County", "")
county_df <- left_join(county_df, test_day, by = "county")

p <-  plot_usmap(include = "TX") +
        geom_polygon(data = county_df, color = "black",
                    aes(x = x, y = y, group = group, fill = county_max,
                        text = paste("County:", county, "<br>Ozone:", county_max))) +
        scale_fill_gradient2(low = "green", high = "red", midpoint = 75, mid = "#ff9200", limit = c(0, 120)) +
        labs(fill = "Ozone Level")

ggplotly(p, tooltip = "text")


Sys.time() - start_time
```


```{r plotly map with geojson}
# url <- 'https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json'
# counties <- rjson::fromJSON(file=url)

# test_day = daily_max_counties %>% filter(date_local == "2020-01-01") %>% select(county, fips, county_max)
# g <- list(
#   scope = 'usa',
#   projection = list(type = 'albers usa'),
#   showlakes = TRUE,
#   showland = TRUE,
#   lakecolor = toRGB('blue'), 
#   landcolor = toRGB("grey90")
#   
# )
# 
# fig <- plot_ly()
# fig <- fig %>% add_trace(
#     type = "choropleth",
#     geojson = counties,
#     locations = test_day$fips,
#     z = test_day$county_max,
#     zmin = 0,  
#     marker = list(line = list(width = 0))
#   )
# 
# fig <- fig %>% colorbar(title = "Ozone Level")
# fig <- fig %>% layout(title = "Ozone Level by County")
# 
# fig <- fig %>% layout(
#     geo = g
#   )
# 
# fig

```


```{r plotly map but with maps data}
start_time <- Sys.time()


test_day = daily_max_counties %>% filter(date_local == "2021-03-17") %>% select(county, fips, county_max)
#
county_df <- map_data("county") %>% filter(region == "texas")
county_df <- county_df %>% rename(state = region, county = subregion)
state_df <- map_data("state") %>% filter(region == "texas")
state_df <- state_df %>% rename(state = region, county = subregion)

county_data <- test_day %>% mutate(county = tolower(county)) %>%
  left_join(county_df, ., by = "county")

p <- ggplot(data = state_df, aes(long, lat, group = group)) +
        coord_fixed(1.15) +
        geom_polygon(fill = "gray")  +
        geom_polygon(data = county_data, color = "black", aes(fill = county_max, text = paste("County:", tools::toTitleCase(county), "<br>Ozone:", county_max))) +
        scale_fill_gradient(low = "green", high = "red", labels = "70 (High)", breaks = 70) +
        geom_polygon(color = "black", fill = NA) + 
        theme_void()

ggplotly(p, tooltip = "text")

Sys.time() - start_time

```



#-------------------  Hourly Analysis Viability -------------------------


```{r county average hourly}

hourly_avg <- ozone_texas %>% group_by(county, date_local, time_local) %>% 
              summarise( # date_time = as.POSIXct(paste(date_local, time_local), format = "%Y-%m-%d %H:%M"), 
                        hourly_avg = mean(sample_measurement, na.rm = TRUE) * 1000)
                                                                                                        
daily_avg <- ozone_texas %>% group_by(county, date_local) %>% 
              summarise( # date_time = as.POSIXct(paste(date_local, time_local), format = "%Y-%m-%d %H:%M"), 
                        daily_avg = mean(sample_measurement, na.rm = TRUE) * 1000)

```



```{r}

hourly_harris <- hourly_avg %>% filter(county == "Harris") %>%
      mutate(date_time = as.POSIXct(paste(date_local, time_local), format = "%Y-%m-%d %H:%M"))

p <- hourly_harris %>% ggplot(aes(x = date_time, y = hourly_avg)) +
       geom_line(color = "blue")
ggplotly(p, dynamicTicks = TRUE) %>% rangeslider() %>% layout(hovermode = "x")




daily_harris <- daily_avg %>% filter(county == "Harris") %>% ungroup()

p <- daily_harris %>% ggplot(aes(x = date_local, y = daily_avg)) +
       geom_line(color = "blue")
ggplotly(p, dynamicTicks = TRUE) %>% rangeslider() %>% layout(hovermode = "x")

```



```{r 8hr max avg vs daily avg}

p <- daily_harris %>% ggplot(aes(x = date_local)) +
       geom_line(aes(y = daily_avg), color = "red") +
       geom_line(aes(y = harris_county$county_max), color = "purple")
ggplotly(p, dynamicTicks = TRUE) %>% rangeslider() %>% layout(hovermode = "x")

cor(harris_county$county_max, daily_harris$daily_avg)
is.na(ozone_texas$sample_measurement) %>% sum() / nrow(ozone_texas)
```


Out of curiosity, I decided to explore whether using the daily average ozone levels would be a better value to try to form a predictive model with. Given that the ozone values will not be maximized, I expect the daily averages to exhibit a smoother transition from day to day when compared to the daily maximum 8-hour average. Comparing the 2 values with respect to time shows they display similar trends, with some exceptions. There are a few days where there are significantly higher spikes from the previous day compared to the increase in daily average. Overall, at first glance there doesnt seem to be a clear reason to prefer the daily average over the maximum 8-hr average given their similarity and high correlation. 

That does leave us with the question of whether we should consider modeling the hourly ozone levels. The glaring downsides of this approach are that there are missing values from sites on certain days and the computational cost of modeling 24 times the data (more of a limitation on my computer's part)



# Counting the number of days the max 8hr avg was in unhealhy territory

## Option 1 is to simply count the number of days, but that would be for the 3 year span.
## May be better to get the year from the date column, group the values by year, and count the number of violations per year to display in the shiny app with the trend plot


```{r ozone violations}

daily_max_counties %>% mutate(year = year(date_local)) %>% group_by(county, year) %>% 
  summarize(num_unhealthy = sum(county_max >= 70)) %>% 
  ungroup() %>% group_by(county) %>% mutate(total_violations = sum(num_unhealthy))
  

```


```{r}
stop()
```

#-------------------------  Modeling  -------------------------#


Next, I will attempt to fit a model the data using a more appropriate method. The classical approach to time series data is to model it using an ARIMA model, or some variation of it, to forecast with it. 


```{r testing arima model}
# Tried to fit an arima model using the auto.arima function, knowing it wasn't going to perform well. But damn, wasn't expecting it to be this bad.

harris_county <- daily_max_counties %>% ungroup() %>% filter(county == "Harris") %>% select(date_local, county_max)
fit = auto.arima(harris_county$county_max)
fit
pred = forecast(fit, 100)
p <- ggplot(harris_county, aes(x = date_local)) +
  geom_line(aes(y = county_max, color = "blue")) + 
  geom_line(aes(y = pred$fitted, color = "red")) +
  scale_color_manual(name="Values",labels=c("Measured Values", "Predictions"), values=c("dodgerblue4", "firebrick4"))
ggplotly(p, dynamicTicks = TRUE) %>% layout(hovermode = "x")

p <- ggplot(harris_county, aes(x = date_local, y = pred$residuals)) +
  geom_line() +
  xlab("Date") + ylab("Residuals")
ggplotly(p, dynamicTicks = TRUE) %>% layout(hovermode = "x")


```


```{r arima model with differencing}

ozone_diff <- data.frame(date_local = harris_county$date_local[-1], diff = diff(harris_county$county_max))
ggplot(ozone_diff, aes(diff)) + geom_histogram()
ggplot(ozone_diff) + geom_line(aes(date_local, diff))

p <- ozone_diff %>% ggplot(aes(date_local, diff)) +
  geom_line(aes(color = "Difference")) +
      geom_smooth(aes(color = "Smoothing funciton")) +
      scale_color_manual(name = "legend", values = c("#69b3a2", "#00A9FF")) +
      xlab("Date") +
      ylab("Ozone Difference") +
      ggtitle("Ozone Daily Change")
ggplotly(p, dynamicTicks = TRUE) %>%  layout(hovermode = "x") 


t.test(ozone_diff$diff)
adf.test(ozone_diff$diff, k = 365)
# Box.test(ozone_diff$diff, type = "Ljung", lag = 365)

par(mfcol = c(2,1))
Acf(ozone_diff$diff, lag.max = 720)
Pacf(ozone_diff$diff, lag.max = 720)
par(mfcol = c(1,1))


# fit an arima model
fit = auto.arima(ozone_diff$diff)
fit
pred = forecast(fit, 100)
p <- ggplot(harris_county, aes(x = date_local)) +
  geom_line(aes(y = county_max, color = "blue")) + 
  geom_line(aes(y = c(78, pred$fitted + 78), color = "red")) +
  scale_color_manual(name="Values",labels=c("Measured Values", "Predictions"), values=c("dodgerblue4", "firebrick4"))
ggplotly(p, dynamicTicks = TRUE) %>% layout(hovermode = "x")

p <- ggplot(ozone_diff, aes(x = date_local, y = pred$residuals)) +
  geom_line() +
  xlab("Date") + ylab("Residuals")
ggplotly(p, dynamicTicks = TRUE) %>% layout(hovermode = "x")
```

Difference statistics are misleading. While hinting at stationarity and other wanteed atrributes in time series data, it performs awful and is no where near the actual values. 

Differencing the data yields some interesting results. Performing an Augmented Dickey-Fuller test shows the data is now stationary, but the Box-Ljung test suggests the 

Time series analysis relies around around the concept of the ideal scenario involving stationary data. Many of the results 



```{r adjusting for seasonality}

ozone_season <- data.frame(diff = diff(harris_county$county_max, lag = 365))
ggplot(ozone_season, aes(diff)) + geom_histogram()

p <- ozone_season %>% ggplot(aes(1:length(diff), diff)) +
      geom_line(aes(color = "Difference")) +
      geom_smooth(aes(color = "Smoothing funciton")) +
      scale_color_manual(name = "legend", values = c("#69b3a2", "#00A9FF")) +
      xlab("Date") +
      ylab("Ozone Difference") +
      ggtitle("Ozone Daily Change")
ggplotly(p, dynamicTicks = TRUE) %>%  layout(hovermode = "x") 



t.test(ozone_season$diff)
adf.test(ozone_season$diff, k = 365)
Box.test(ozone_season$diff, type = "Ljung")

par(mfcol = c(2,1))
Acf(ozone_season$diff, lag.max = 720)
Pacf(ozone_season$diff, lag.max = 720)
par(mfcol = c(1,1))

```
