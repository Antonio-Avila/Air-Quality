---
title: "AQS"
author: "Antonio Avila"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(readxl)
library(RAQSAPI)
library(keyring)
library(beepr)
library(usmap)
library(lubridate)
library(forecast)
library(plotly)
library(rjson)
library(forecast)
library(tseries)
library(quantmod)
library(maps)
library(RColorBrewer)
```

## R Markdown

```{r enable AQS credentials, include = FALSE}
# key_set(service = 'AQSDatamart', username = 'aavila2802@gmail.com')

datamart_user <-  'aavila2802@gmail.com'
server <-  'AQSDatamart'
aqs_credentials(username = datamart_user, key_get(service = server, username = datamart_user))
```


```{r state codes, include = FALSE}

state_codes <- aqs_states()

```


```{r extract ozone data, include = FALSE}

# parameter 62104 is the max temp in a 24 Hr period
# parameter 62105 is the min temp in a 24 Hr period
# parameter 62106 is the temp diff in a 24 Hr period
# parameter 44201 is the ozone 
# parameter 42102 is carbon dioxide

# Working for the parameter code 45201 (benzine) but not for any other codes from the list above

# start_time = proc.time()
# ozone_texas <- aqs_sampledata_by_state(parameter = "44201",
#                                            bdate = as.Date("20200901",
#                                                            format="%Y%m%d"
#                                                            ),
#                                            edate = as.Date("20210901",
#                                                           format = "%Y%m%d"),
#                                            stateFIPS = "48"
#                                           )
# print("Time to Execute:")
# print(proc.time() - start_time)
# beep()

```


```{r extract pm2.5 data}
# Sys.sleep(120)
# start_time = Sys.time()
# pm2.5_texas <- aqs_sampledata_by_state(parameter = "88101",
#                                            bdate = as.Date("20170901",
#                                                            format="%Y%m%d"
#                                                            ),
#                                            edate = as.Date("20180901",
#                                                           format = "%Y%m%d"),
#                                            stateFIPS = "48"
#                                           )
# write_csv(pm2.5_texas, "pm2.5_texas_2017.csv")
# Sys.time() - start_time
# beep()
```


```{r clean extrated data}

# ozone_texas_20_21 <- read_csv("ozone_texas_20_21.csv")
# ozone_texas_19_20 <- read_csv("ozone_texas_19_20.csv")
# ozone_texas_18_19 <- read_csv("ozone_texas_18_19.csv")
# ozone_texas_17_18 <- read_csv("ozone_texas_17_18.csv")
# 
# ozone_texas <- union(union(union(ozone_texas_17_18, ozone_texas_18_19), ozone_texas_19_20), ozone_texas_20_21)
# write_csv(ozone_texas, "ozone_texas.csv")
# 
# pm2.5_texas_2020 <- read_csv("pm2.5_texas_2020.csv")
# pm2.5_texas_2019 <- read_csv("pm2.5_texas_2019.csv")
# pm2.5_texas_2018 <- read_csv("pm2.5_texas_2018.csv")
# pm2.5_texas_2017 <- read_csv("pm2.5_texas_2017.csv")

# pm2.5_texas <- bind_rows(pm2.5_texas_2017, pm2.5_texas_2018, pm2.5_texas_2019, pm2.5_texas_2020) %>% distinct(.keep_all = TRUE)
# write_csv(pm2.5_texas, "pm2.5_texas.csv"



# Organize Covid data

# covid_cases_2020 <- read_excel("texas_covid_cases.xlsx", skip = 2) %>% select(-`Unknown Date`, -`2020 Total`) %>% slice(1:(n() - 6)) 
# covid_cases_2020 <- covid_cases_2020 %>% pivot_longer(!County, names_to = "Date", values_to = "Cases")
# covid_cases_2020$Date <- mdy(covid_cases_2020$Date)
# 
# covid_cases_2021 <- read_excel("texas_covid_cases.xlsx", sheet = 2, skip = 2) %>% slice(1:(n() - 2)) 
# covid_cases_2021 <- covid_cases_2021 %>% pivot_longer(!County, names_to = "Date", values_to = "Cases")
# covid_cases_2021$Date <- as.Date(as.numeric(covid_cases_2021$Date), origin = "1899-12-30")
# 
# covid_cases <- bind_rows(covid_cases_2020, covid_cases_2021)
# write_csv(covid_cases, "covid_cases.csv")
# 
# path_fatal <- "texas_covid_fatalities.xlsx"
# covid_fatal_2020 <- read_excel(path_fatal, skip = 2, sheet = 1) %>% slice(1:(n() - 2)) 
# covid_fatal_2020 <- covid_fatal_2020 %>% pivot_longer(!County, names_to = "Date", values_to = "Fatalities")
# covid_fatal_2020$Date <- mdy(covid_fatal_2020$Date)
# 
# covid_fatal_2021 <- read_excel(path_fatal, skip = 2, sheet = 2) %>% slice(1:(n() - 2)) 
# covid_fatal_2021 <- covid_fatal_2021 %>% pivot_longer(!County, names_to = "Date", values_to = "Fatalities")
# covid_fatal_2021$Date <- mdy(covid_fatal_2021$Date)
# 
# covid_fatal <- bind_rows(covid_fatal_2020, covid_fatal_2021)
# write_csv(covid_fatal, "covid_fatal.csv")

```


```{r import data}

# Import Ozone data
ozone_texas <- read_csv("ozone_texas.csv")
ozone_texas["fips"] <- as.integer(ozone_texas$state_code) * 1000 + as.integer(ozone_texas$county_code)
ozone_texas["date_time_local"] <- ymd(ozone_texas$date_local) + hms(ozone_texas$time_local)


# Import PM2.5 data 
pm2.5_texas <- read_csv("pm2.5_texas.csv")
pm2.5_texas["fips"] <- as.integer(pm2.5_texas$state_code) * 1000 + as.integer(pm2.5_texas$county_code)
pm2.5_texas["date_time_local"] <- ymd(pm2.5_texas$date_local) + hms(pm2.5_texas$time_local)


# Import COVID data
covid_cases <- read_csv("covid_cases.csv")
covid_cases <- covid_cases %>% select(Date, County, Cases)
covid_cases$County <- as.factor(covid_cases$County)
covid_fatal <- read_csv("covid_fatal.csv")
covid_fatal <- covid_fatal %>% select(Date, County, Fatalities)
covid_fatal$County <- as.factor(covid_fatal$County)

```


#---------------------------------------------------- Ozone Analysis ----------------------------------------------------#

```{r function for 8 hour ozone avg}


avg_8hr <- function(data){
  avgs = matrix(nrow = 24-8, ncol = 1)
  for(i in 1:(24-8)) { 
    avgs[i] = mean(data[i:(i+7)], na.rm = TRUE)
  }
  return(avgs)
}


max_8hr_avg <- function(data){
  max_avg = max(avg_8hr(data)) * 1000
  max_avg = trunc(max_avg)
  return(max_avg)
}


```


```{r verify with published data}
# Verified calculations with results from various days published by the Texas Commission on Environmental Quality
# 
# test_day <- ozone_texas %>% filter(date_local == "2019-10-02", site_number == "0416")
# test_day$sample_measurement %>% max_8hr_avg()

```



```{r find daily max avg per county}
daily_max_sites <- ozone_texas %>% 
  group_by(longitude, latitude, county, fips, site_number, date_local) %>% 
  summarize(max_8hr_avg = max_8hr_avg(sample_measurement)) %>% ungroup()

daily_max_counties <- daily_max_sites %>%
  group_by(county, fips, date_local) %>% 
  summarise(ozone_avg = max(max_8hr_avg, na.rm = TRUE)) %>% ungroup()
daily_max_counties[is.infinite(daily_max_counties$ozone_avg),]$ozone_avg <- NA

# write_csv(daily_max_counties, "ozone_daily_max_counties.csv")
```



```{r Harris county ozone trend plot, include = FALSE}

# Normal gpplot, but working towards turning it into an interactive plot_ly plot 

# p <- daily_max_counties %>% filter(county == "Harris") %>% 
#   ggplot(aes(date_local, county_max)) + 
#     geom_line(aes(color = county_max)) + 
#     scale_color_gradient(low = "green", high = "red") +
#     geom_hline(yintercept = 70, color = "red") +
#     geom_smooth(method = "gam", se = FALSE) + 
#     ggtitle("Harris County Ozone levels 2017-2021") +
#     xlab("Date") +
#     ylab("Ozone 8-Hr Max (ppb)") +
#     annotate("text", x = as.Date("2018-01-01"), y = 71, label = "Unhealthy Level")
# p


p <- daily_max_counties %>% filter(county == "Harris") %>% 
  ggplot(aes(date_local, county_max)) + 
    geom_line(aes(color = "8-Hr Max Avg Ozone")) + 
    geom_hline(yintercept = 70, color = "red") +
    geom_smooth(method = "gam", se = FALSE, aes(color = "Local Model")) + 
    geom_smooth(method = "lm", aes(color = "Trend"), se = FALSE) +
    ggtitle( paste("Harris", "County Ozone levels 2017-2021")) +
    xlab("Date") +
    ylab("Ozone 8-Hr Max (ppb)") +
    annotate("text", x = as.Date("2018-01-01"), y = 71, label = "Unhealthy Level") +
    scale_color_manual(name = "Legend", values = c("#69b3a2", "#00A9FF", "purple"))
ggplotly(p, dynamicTicks = TRUE) %>%  layout(hovermode = "x") %>% rangeslider()

ozone_lm <- lm(county_max ~., data = harris_county)
summary(ozone_lm)


# Some statistics
adf.test(harris_county$county_max, k = 365)
Box.test(harris_county$county_max, type = "Ljung", lag = 365)

par(mfcol = c(2,1))
Acf(harris_county$county_max, lag.max = 720)
Pacf(harris_county$county_max, lag.max = 720)
par(mfcol = c(1,1))
```



Even though there is some variability throughout the time period in the daily, maximum 8-hour ozone levels, the plot suggests there may be a slight downward trend. Fitting a linear model to the time series data rejects this notion. The p-value for the coefficient corresponding to the date yields a value of 0.47, meaning the coefficient and thus variable is not statistically significant. There is no evidence to suggest a statistically significant decrease in ozone levels with time. Granted, this model cannot be used for anything other than determining a trend since it is a very poor fit for the data. The data is obviously non-linear with some seasonality sprinkled in there, which makes sense since ozone levels should increase during summer periods. 


#----------------------  Mapping  ----------------------



```{r ozone heat map}
start_time <- Sys.time()

palette <- brewer.pal(11, "RdYlGn")

test_day <- daily_max_counties %>% filter(date_local == "2018-08-27")
county_df <- us_map("county") %>% filter(full == "Texas") %>% rename(state = full)
county_df$county <- str_replace(county_df$county, " County", "")
county_df <- left_join(county_df, test_day, by = "county")

p <-  plot_usmap(include = "TX") +
        geom_polygon(data = county_df, color = "black",
                    aes(x = x, y = y, group = group, fill = county_max,
                        text = paste("County:", county, "<br>Ozone:", county_max))) +
        # scale_fill_gradient2(low = "green", high = "red", mid = "orange", midpoint = 65, limit = c(0, 120)) +
        scale_fill_gradientn(colors = rev(palette),  limit = c(0, 120)) +
        labs(fill = "Ozone Level")

ggplotly(p, tooltip = "text")


Sys.time() - start_time
```


```{r plotly map with geojson}
# url <- 'https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json'
# counties <- rjson::fromJSON(file=url)

# test_day = daily_max_counties %>% filter(date_local == "2020-01-01") %>% select(county, fips, county_max)
# g <- list(
#   scope = 'usa',
#   projection = list(type = 'albers usa'),
#   showlakes = TRUE,
#   showland = TRUE,
#   lakecolor = toRGB('blue'), 
#   landcolor = toRGB("grey90")
#   
# )
# 
# fig <- plot_ly()
# fig <- fig %>% add_trace(
#     type = "choropleth",
#     geojson = counties,
#     locations = test_day$fips,
#     z = test_day$county_max,
#     zmin = 0,  
#     marker = list(line = list(width = 0))
#   )
# 
# fig <- fig %>% colorbar(title = "Ozone Level")
# fig <- fig %>% layout(title = "Ozone Level by County")
# 
# fig <- fig %>% layout(
#     geo = g
#   )
# 
# fig

```


```{r plotly map but with maps data}
start_time <- Sys.time()


test_day = daily_max_counties %>% filter(date_local == "2021-03-17") %>% select(county, fips, county_max)
#
county_df <- map_data("county") %>% filter(region == "texas")
county_df <- county_df %>% rename(state = region, county = subregion)
state_df <- map_data("state") %>% filter(region == "texas")
state_df <- state_df %>% rename(state = region, county = subregion)

county_data <- test_day %>% mutate(county = tolower(county)) %>%
  left_join(county_df, ., by = "county")

p <- ggplot(data = state_df, aes(long, lat, group = group)) +
        coord_fixed(1.15) +
        geom_polygon(fill = "gray")  +
        geom_polygon(data = county_data, color = "black", aes(fill = county_max, text = paste("County:", tools::toTitleCase(county), "<br>Ozone:", county_max))) +
        scale_fill_gradient(low = "green", high = "red", labels = "70 (High)", breaks = 70) +
        geom_polygon(color = "black", fill = NA) + 
        theme_void()

ggplotly(p, tooltip = "text")

Sys.time() - start_time

```



#-------------------  Hourly Analysis Viability -------------------------

Out of curiosity, I decided to explore whether using the daily average ozone levels would be a better value to try to form a predictive model with. Given that the ozone values will not be maximized, I expect the daily averages to exhibit a smoother transition from day to day when compared to the daily maximum 8-hour average. Comparing the 2 values with respect to time shows they display similar trends, with some exceptions. There are a few days where there are significantly higher spikes from the previous day compared to the increase in daily average. Overall, at first glance there doesnt seem to be a clear reason to prefer the daily average over the maximum 8-hr average given their similarity and high correlation. 

That does leave us with the question of whether we should consider modeling the hourly ozone levels. The glaring downsides of this approach are that there are missing values from sites on certain days and the computational cost of modeling 24 times the data (more of a limitation on my computer's part)

```{r county average hourly}

hourly_avg <- ozone_texas %>% group_by(county, date_local, time_local) %>% 
              summarise( # date_time = as.POSIXct(paste(date_local, time_local), format = "%Y-%m-%d %H:%M"), 
                        hourly_avg = mean(sample_measurement, na.rm = TRUE) * 1000)
                                                                                                        
daily_avg <- ozone_texas %>% group_by(county, date_local) %>% 
              summarise( # date_time = as.POSIXct(paste(date_local, time_local), format = "%Y-%m-%d %H:%M"), 
                        daily_avg = mean(sample_measurement, na.rm = TRUE) * 1000)

```



```{r}

hourly_harris <- hourly_avg %>% filter(county == "Harris") %>%
      mutate(date_time = as.POSIXct(paste(date_local, time_local), format = "%Y-%m-%d %H:%M"))

p <- hourly_harris %>% ggplot(aes(x = date_time, y = hourly_avg)) +
       geom_line(color = "blue")
ggplotly(p, dynamicTicks = TRUE) %>% rangeslider() %>% layout(hovermode = "x")




daily_harris <- daily_avg %>% filter(county == "Harris") %>% ungroup()

p <- daily_harris %>% ggplot(aes(x = date_local, y = daily_avg)) +
       geom_line(color = "blue")
ggplotly(p, dynamicTicks = TRUE) %>% rangeslider() %>% layout(hovermode = "x")

```



```{r 8hr max avg vs daily avg}

p <- daily_harris %>% ggplot(aes(x = date_local)) +
       geom_line(aes(y = daily_avg), color = "red") +
       geom_line(aes(y = harris_county$county_max), color = "purple")
ggplotly(p, dynamicTicks = TRUE) %>% rangeslider() %>% layout(hovermode = "x")

cor(harris_county$county_max, daily_harris$daily_avg)
is.na(ozone_texas$s;ample_measurement) %>% sum() / nrow(ozone_texas)
```





 Counting the number of days the max 8hr avg was in unhealhy territory

- Option 1 is to simply count the number of days, but that would be for the 3 year span.
- May be better to get the year from the date column, group the values by year, and count the number of violations per year to display in the shiny app with the trend plot


```{r ozone violations}

daily_max_counties %>% mutate(year = year(date_local)) %>% group_by(county, year) %>% 
  summarize(num_unhealthy = sum(county_max >= 70)) %>% 
  ungroup() %>% group_by(county) %>% mutate(total_violations = sum(num_unhealthy))
  

```


```{r}
stop()
```

#-------------------------  Modeling  -------------------------#


Next, I will attempt to fit a model the data using a more appropriate method. The classical approach to time series data is to model it using an ARIMA model, or some variation of it, to forecast with it. 


```{r testing arima model}
# Tried to fit an arima model using the auto.arima function, knowing it wasn't going to perform well. But damn, wasn't expecting it to be this bad.

harris_county <- daily_max_counties %>% ungroup() %>% filter(county == "Harris") %>% select(date_local, county_max)
fit = auto.arima(harris_county$county_max)
fit
pred = forecast(fit, 100)
p <- ggplot(harris_county, aes(x = date_local)) +
  geom_line(aes(y = county_max, color = "blue")) + 
  geom_line(aes(y = pred$fitted, color = "red")) +
  scale_color_manual(name="Values",labels=c("Measured Values", "Predictions"), values=c("dodgerblue4", "firebrick4"))
ggplotly(p, dynamicTicks = TRUE) %>% layout(hovermode = "x")

p <- ggplot(harris_county, aes(x = date_local, y = pred$residuals)) +
  geom_line() +
  xlab("Date") + ylab("Residuals")
ggplotly(p, dynamicTicks = TRUE) %>% layout(hovermode = "x")


```


```{r arima model with differencing}

ozone_diff <- data.frame(date_local = harris_county$date_local[-1], diff = diff(harris_county$county_max))
ggplot(ozone_diff, aes(diff)) + geom_histogram()
ggplot(ozone_diff) + geom_line(aes(date_local, diff))

p <- ozone_diff %>% ggplot(aes(date_local, diff)) +
  geom_line(aes(color = "Difference")) +
      geom_smooth(aes(color = "Smoothing funciton")) +
      scale_color_manual(name = "legend", values = c("#69b3a2", "#00A9FF")) +
      xlab("Date") +
      ylab("Ozone Difference") +
      ggtitle("Ozone Daily Change")
ggplotly(p, dynamicTicks = TRUE) %>%  layout(hovermode = "x") 


t.test(ozone_diff$diff)
adf.test(ozone_diff$diff, k = 365)
# Box.test(ozone_diff$diff, type = "Ljung", lag = 365)

par(mfcol = c(2,1))
Acf(ozone_diff$diff, lag.max = 720)
Pacf(ozone_diff$diff, lag.max = 720)
par(mfcol = c(1,1))


# fit an arima model
fit = auto.arima(ozone_diff$diff)
fit
pred = forecast(fit, 100)
p <- ggplot(harris_county, aes(x = date_local)) +
  geom_line(aes(y = county_max, color = "blue")) + 
  geom_line(aes(y = c(78, pred$fitted + 78), color = "red")) +
  scale_color_manual(name="Values",labels=c("Measured Values", "Predictions"), values=c("dodgerblue4", "firebrick4"))
ggplotly(p, dynamicTicks = TRUE) %>% layout(hovermode = "x")

p <- ggplot(ozone_diff, aes(x = date_local, y = pred$residuals)) +
  geom_line() +
  xlab("Date") + ylab("Residuals")
ggplotly(p, dynamicTicks = TRUE) %>% layout(hovermode = "x")
```

Difference statistics are misleading. While hinting at stationarity and other wanted attributes in time series data, it performs awful and is no where near the actual values. 

Differencing the data yields some interesting results. Performing an Augmented Dickey-Fuller test shows the data is now stationary, but the Box-Ljung test suggests the 

Time series analysis relies around around the concept of the ideal scenario involving stationary data. Many of the results 



```{r adjusting for seasonality}

ozone_season <- data.frame(diff = diff(harris_county$county_max, lag = 365))
ggplot(ozone_season, aes(diff)) + geom_histogram()

p <- ozone_season %>% ggplot(aes(1:length(diff), diff)) +
      geom_line(aes(color = "Difference")) +
      geom_smooth(aes(color = "Smoothing funciton")) +
      scale_color_manual(name = "legend", values = c("#69b3a2", "#00A9FF")) +
      xlab("Date") +
      ylab("Ozone Difference") +
      ggtitle("Ozone Daily Change")
ggplotly(p, dynamicTicks = TRUE) %>%  layout(hovermode = "x") 



t.test(ozone_season$diff)
adf.test(ozone_season$diff, k = 365)
Box.test(ozone_season$diff, type = "Ljung")

par(mfcol = c(2,1))
Acf(ozone_season$diff, lag.max = 720)
Pacf(ozone_season$diff, lag.max = 720)
par(mfcol = c(1,1))

```




#---------------------------------------------------- PM2.5 Analysis ----------------------------------------------------#

When first exploring the data, I discovered that PM2.5 measurements were not being taken consistently initially. The reason for this is that in Texas there weren't many sites recording PM2.5 data in 2018 but have since quadrupled to about 40 sites in 2021. 

```{r number of sites}
p <- pm2.5_texas[!is.na(pm2.5_texas$sample_measurement), ] %>% group_by(date_local) %>% summarise(n = n_distinct(site_number)) %>% 
  ggplot(aes(date_local, n)) + 
    geom_point() +
    geom_smooth(se = FALSE) +
    labs(title = "Number of Sites recording PM2.5 Data") +
    xlab("Date") + ylab("Number of Sites")
ggplotly(p, dynamicTicks = TRUE)
```


Next, I thought it would be beneficial to look at how the PM2.5 levels on average evolve throughout the day to look for any trends.  As expected, levels tend to peak twice, first around 9:00 A.M. then again at 8:00 - 9:00 P.M. but to a lesser extent. This make sense since it lines up with the commute times of people going into work and back home. This does not reflect or display any seasonal differences that may be present which is something I want to look into in the future.


```{r avergage pm2.5 throughout day}

pm2.5_texas %>% group_by(time_local) %>% 
  summarize(hourly_avg = mean(sample_measurement, na.rm = TRUE), hourly_med = median(sample_measurement, na.rm = TRUE)) %>% 
      ggplot(aes(time_local)) + geom_line(aes(y = hourly_avg)) + geom_line(aes(y = hourly_med), color = "blue")

```



Need to continue investigating why there are negative values (likely due to equipment malfunction or dropping values below detection limit)
```{r investigate data quality}

weird_vals <- pm2.5_texas %>% filter(sample_measurement < 0)
weird_vals %>% select(county, site_number, date_local, sample_measurement) %>% head(20)


weird_vals %>% count(county, site_number)  
weird_vals %>% count(qualifier)

```



In reality, it is impossible to have negative particulate matter (PM2.5) levels. So it is likely the values are due to some kind of recording error caused by the machine. After a little investigation, I noticed that outages or malfunctions are cataloged and the values are removed when the data is being verified. Typically there are notes included for data removal. The EPA does allow monitoring sites to report negative values so long as they are above a certain threshold (> -10 ug/m3), for data reported to the AQS database. However, it does not specify what should be done with the data when analyzing. 

After much digging, the only information I found on negative particulate matter measurements came from a National Ambient Air Monitoring Conference in 2016 hosted by the EPA. 
According to the conference, negative values can occur as the measurements fall below the detection limit of the device and approach 0.  They should be reported by monitoring sites but should not be included in public reports. Moreover, excluding those points or replacing them with 0 would lead to an increase in bias in the data. Perhaps a compromise would be to replace the negative values with half of the detection limit. 


```{r replace negatives with half DL}

# Code to replace negative values


```


Now onto investigating missing values and what their qualifiers denote about the data.

As suspected, the reason why values are missing are logged appropriately, which makes my life so much easier since I dont have to investigate potential causes. On the other hand, there are values that are recorded that have some kind of qualifier, such as African dust being present or construction occurring near the instrument. I wonder if these are inflating the PM2.5 values. 


```{r qualifiers}

# missing_vals <- pm2.5_texas %>% filter(is.na(sample_measurement))
# missing_vals %>% select(county, site_number, date_local, sample_measurement, qualifier) %>% count(qualifier) %>% View()

pm2.5_texas %>% count(missing_val = is.na(sample_measurement), qualifier) %>% 
  ggplot(aes(x = qualifier, y = n, fill = missing_val)) +
    # facet_grid(rows = vars(missing_val)) +
    geom_bar(stat = "identity", position = "dodge") +
    coord_flip()


# Exclude the non-missing values without a qualifier, the vast majority of samples
pm2.5_texas %>% count(missing_val = is.na(sample_measurement), qualifier) %>% filter(!is.na(qualifier)) %>% 
  ggplot(aes(x = qualifier, y = n, fill = missing_val)) +
    # facet_grid(rows = vars(missing_val)) +
    geom_bar(stat = "identity", position = "stack") +
    coord_flip()
pm2.5_texas %>% count(missing_val = is.na(sample_measurement), qualifier) %>% filter(!is.na(qualifier), missing_val == FALSE)


# 
# pm2.5_texas %>% group_by(date_local, county, site_number) %>% count(!is.na(qualifier))
# 
# pm2.5_texas %>% filter(county == "Harris", sample_duration == "1 HOUR") %>% 
#   ggplot(aes(x = date_local, y = sample_measurement)) +
#     geom_line()

```

There seems to be 22 types of qualifiers which lead to whether a sample measurement is recorded or not. Some of the qualifiers do correspond with sensible reasons to exclude a measurement, whether it be machine errors, maintenance, the machine not recording data, etc. Meanwhile, other types seem to be reasons for the measurements to be potentially elevated, such as African dust, construction nearby, outliers, etc. The qualifiers seem to be mutually exclusice in regards to whether the data was missing or not. 

The most interesting thing the qualifiers reveal is while these event do affect particulate matter levels and push them to unhealthy levels, they dont elevate them to extraordinary levels. In fact, the all of the high values (i.e. over 100 um/3) correspond to the no qualifier category. About half of the categories do elevate the particulate matter levels more than the average day would. Fires, African dust, and samples measured as outliers have a higher level on average.

```{r pm2.5 for non-missing data with qualifier}

pm2.5_texas %>% filter(!is.na(sample_measurement), !is.na(qualifier)) %>% group_by(qualifier) %>% summarize(qual_mean = mean(sample_measurement), qual_sd = sd(sample_measurement))

# Boxplot for qualifiers whos values are measured, excluding those with no qualifier (normal values)
pm2.5_texas %>% filter(!is.na(sample_measurement), !is.na(qualifier)) %>% 
  ggplot(aes(x = sample_measurement, y = qualifier, fill = qualifier)) + 
    geom_boxplot() + 
    theme(legend.position = "none")

# Boxplot for all values categorized by their qualifiers
pm2.5_texas %>% filter(!is.na(sample_measurement)) %>% 
  ggplot(aes(x = sample_measurement, y = qualifier, fill = qualifier)) + 
    geom_boxplot() + 
    theme(legend.position = "none")

# Same as plot above but truncated to better compare (excluding outliers)
pm2.5_texas %>% filter(!is.na(sample_measurement), sample_measurement < 100) %>% 
  ggplot(aes(x = sample_measurement, y = qualifier, fill = qualifier)) + 
    geom_boxplot() + 
    theme(legend.position = "none")


# Bar plot displaying the number of each type of recorded qualifier
pm2.5_texas %>% filter(!is.na(sample_measurement), !is.na(qualifier)) %>% count(qualifier) %>% 
  ggplot(aes(x = n, y = qualifier, fill = qualifier)) + 
    geom_bar(stat = "identity") + 
    theme(legend.position = "none")


# Plot displaying sample trends separated by their qualifiers. (Needs work) for Harris county
p <- pm2.5_texas %>%  filter(county == "Harris") %>% 
  ggplot(aes(x = ymd(date_local) + hms(time_local), y = sample_measurement)) + 
      facet_grid(rows = "site_number") +
      geom_line(aes(color = qualifier))
ggplotly(p, dynamicTicks = TRUE)


quali <- pm2.5_texas$qualifier
quali[is.na(quali)] <- "none"
anova_model <- lm(pm2.5_texas$sample_measurement ~ quali)
summary(anova_model)
```


Unhealthy levels for sensitive groups occur at levels higher than 35, but for everyone in general, it is unsafe when levels reach 55.
Surprisingly, >99% of pm2.5 samples recorded over an unhealthy value have no qualifier, meaning the recordings were correct and there were no external influences that needed to be recorded. The remaining 3 sample measurements were affected by African dust of a prescribed fire, as recorded in the qualifier variable. Moreover, they werent even close to the highest levels recorded, in fact they were marginally unhealthy at a max of about 70 u/m3.
The 5 highest pm2.5 values do have a reasonable possible explanation. One of them corresponds to November 29, 2018 which happened to be Thanksgiving. The high value makes sense since a lot of people tend to travel to spend it with their family. 3 of them correspond to New Years for 2020 and 2019, which also make sense since people also tend to travel and light fireworks, increasing particulate matter due to the debris in the air. The only one that did not make sense was the last date which happens to be the most recent on occurring on March 12, 2021 in Harris County. After a little research, nothing of interest happened that day to cause such a large measurement. Furthermore, it should be noted the measurement was more of a significant spike from the previous and following hour, leading me to believe it is actually a machine error or outlier, whereas in the other cases the high values were led by a build up or followed by a gradual decline. 

```{r unhealthy pm levels}

# Top 5
p <- pm2.5_texas %>% filter(sample_measurement > 55) %>% 
  ggplot(aes(x = date_time_local, y = sample_measurement)) +
    geom_point(aes(color = qualifier))
ggplotly(p, dynamicTicks = TRUE)


nrow(pm2.5_texas %>% filter(sample_measurement > 55, !is.na(qualifier))) / nrow(pm2.5_texas %>% filter(sample_measurement > 55))
nrow(pm2.5_texas %>% filter(sample_measurement > 55, is.na(qualifier))) / nrow(pm2.5_texas %>% filter(sample_measurement > 55))


pm2.5_texas %>% filter(sample_measurement > 500) %>% select(site_number, date_local)
pm2.5_texas %>% filter(date_local == '2018-11-29', site_number == "0314") %>% View()
pm2.5_texas %>% filter(date_local == '2019-01-01', site_number == "1034") %>% View()
pm2.5_texas %>% filter(date_local == '2020-01-01', site_number == "0024") %>% View()
pm2.5_texas %>% filter(date_local == '2021-03-12', site_number == "1039") %>% View()


# Greater than 100
pm2.5_texas %>% filter(sample_measurement > 100) %>% select(county, site_number, date_time_local, sample_measurement, qualifier) %>% 
  ggplot(aes(x = sample_measurement)) + 
    geom_histogram()


p <- pm2.5_texas %>% filter(sample_measurement > 100) %>% count(date_local) %>% 
  ggplot(aes(x = date_local, y = n)) +
    geom_bar(stat = "identity")
ggplotly(p, dynamicTicks = TRUE)
```

While investigating how to handle negative values, I found no guidance or guidelines set by any agency as to how to handle such values, other than they are acceptable to be submitted to the EPA for recording but should not be included in any form of public reporting. 

```{r how many negatives per county}

neg_counts <- pm2.5_texas %>% filter(sample_measurement <= 0) %>% count(county, .drop = TRUE) 
neg_counts <- rbind(neg_county, c("Ellis", 0)) %>% left_join(pm2.5_texas %>% count(county), by = "county")
neg_counts <- neg_counts %>% rename(missing = n.x, total = n.y)
neg_counts$missing <- as.integer(neg_counts$missing)
neg_counts <- neg_counts %>%  mutate(perc_missing = missing / total)


neg_counts %>% 
  ggplot(aes(x = county, y = missing, fill = county)) + 
    geom_bar(stat = "identity")

neg_counts %>% 
  ggplot(aes(x = county, y = perc_missing, fill = county)) + 
  geom_bar(stat = "identity")

```


```{r replace negative}

(pm2.5_texas$sample_measurement <= 0)
pm2.5_texas %>% filter(sample_measurement <= 0) %>% mutate(sample_measurement = detection_limit / 2) %>% select(sample_measurement) %>% count(sample_measurement)

```


```{r daily average pm2.5}

daily_avg_pm2.5_site <- pm2.5_texas %>% filter(sample_duration == "1 HOUR", sample_frequency == "HOURLY") %>% 
  group_by(longitude, latitude, county, fips, site_number, date_local) %>% 
  summarize(site_avg = mean(sample_measurement, na.rm = TRUE)) %>% ungroup()


daily_avg_pm2.5_county <- daily_avg_pm2.5_site %>% group_by(county, fips, date_local) %>% 
  summarise(pm2.5_avg = max(site_avg, na.rm = TRUE)) %>% ungroup()
daily_avg_pm2.5_county[is.infinite(daily_avg_pm2.5_county$pm2.5_avg), ] <- NA

# write_csv(daily_avg_pm2.5_county, "pm2.5_daily_avg_counties.csv")
```



```{r pm trend}

# Harris County
p <- daily_avg_pm2.5_county %>% filter(county == "Harris") %>% 
        ggplot(aes(x = date_local, y = pm2.5_avg)) + 
          geom_line(aes(color = "PM2.5 Daily Avg")) +
          geom_hline(yintercept = 12, color = "red") +
          geom_smooth(method = "gam", aes(color = "Trend"), se = FALSE) +
          ggtitle("Harris County PM2.5 Levels from 2017-2021") +
          xlab("Date") +
          ylab("PM2.5 Concentration (ug/m3)") +
          annotate("text", x = as.Date("2018-08-08"), y = 13, label = "High") +
          scale_color_manual(name = "Legend", values = c("#69b3a2", "purple"))

ggplotly(p, dynamicTicks = TRUE) %>% rangeslider() %>% layout(hovermode = "x")



ldata <- daily_avg_pm2.5_county %>% filter(county == "Harris") %>% select(county_avg) %>% drop_na() 
shapiro.test(log(data[[1]]))
ks.test(log(data[[1]]), "pnorm")
jarque.bera.test(log(data[[1]]))

t.test(x = log(data), mu = log(12), alternative = "greater")
wilcox.test(x = data[[1]], mu = 12, alternative = "greater")


sim_norm = rnorm(nrow(data), mean(log(data[[1]])), sd(log(data[[1]])))
data %>% ggplot(aes(log(county_avg))) + geom_histogram(fill = alpha("red", 0.5)) + geom_histogram(aes(sim_norm), fill = alpha("blue", 0.3))

```



```{r}

p <- daily_avg_pm2.5_county %>% filter(county == "Harris") %>% mutate(year = year(date_local), dm = mday(date_local)) %>% 
        ggplot(aes(x = date_local, y = pm2.5_avg)) + 
          geom_line(aes(color = year)) +
          geom_hline(yintercept = 12, color = "red") +
          geom_smooth(method = "gam", aes(color = "Trend"), se = FALSE) +
          ggtitle("Harris County PM2.5 Levels from 2017-2021") +
          xlab("Date") +
          ylab("PM2.5 Concentration (ug/m3)") +
          annotate("text", x = as.Date("2018-08-08"), y = 13, label = "High") +
          scale_color_manual(name = "Legend", values = c("#69b3a2", "purple"))

ggplotly(p, dynamicTicks = TRUE) %>% rangeslider() %>% layout(hovermode = "x")





test_dat <- daily_avg_pm2.5_county %>% filter(county == "Harris", date_local != "2020-02-29") %>% mutate(year_cov = as.factor(year(date_local))) %>% filter(year_cov %in% c(2018, 2019, 2020)) 
test_dat["index"] <- rep(1:365, 3)


p <- test_dat %>% ggplot(aes(x = index, y = pm2.5_avg)) + geom_smooth(aes(color = year_cov), se = FALSE, span = 0.5) + geom_line(aes(color = year_cov))
ggplotly(p, dynamicTicks = TRUE)  %>% layout(hovermode = "x")


p <- test_dat %>% ggplot(aes(x = index, y = pm2.5_avg, color = year_cov)) + geom_smooth(se = FALSE) 
ggplotly(p, dynamicTicks = TRUE)  %>% layout(hovermode = "x")
```


```{r test arima}

test_data <- pm2.5_texas %>% filter(county == "Harris", site_number == "0058") %>% group_by(date_local) %>% summarise(pm_avg = mean(sample_measurement, na.rm = TRUE))
adf.test(na.remove(test_data$pm_avg))
adf.test(diff(na.remove(test_data$pm_avg)))

Acf(test_data$pm_avg)
Pacf(test_data$pm_avg)


test_model <- auto.arima(test_data$pm_avg)
summary(test_model)
adf.test(na.remove(test_model$residuals))
t.test(test_model$residuals)

Acf(test_model$residuals)
Pacf(test_model$residuals)

preds <- forecast(test_model, 10)
preds
plot(preds)

# ggplot(test_data, aes(x = date_local, y = data.frame(preds$fitted))) + geom_line()
autoplot(preds) +
  xlim(1400, 1490)


p <- test_data[6:nrow(test_data), ] %>% ggplot(aes(x = date_local, y = pm_avg)) + 
  geom_line(color = "blue")  + 
  geom_line(aes(y = preds$fitted), color = "red")
ggplotly(p, dynamicTicks = TRUE)  %>% layout(hovermode = "x")

```


```{r test arima on diff data}

test_data <- pm2.5_texas %>% filter(county == "Harris", site_number == "0058") %>% group_by(date_local) %>% summarise(pm_avg = mean(sample_measurement, na.rm = TRUE))

diff_data <- data.frame(time = test_data$date_local[2:nrow(test_data)], diff_pm = diff(test_data$pm_avg))
adf.test(na.remove(diff_data$diff_pm))

Acf(diff_data$diff_pm)
Pacf(diff_data$diff_pm)

model_diff <- auto.arima(diff_data$diff_pm)
summary(model_diff)

Acf(model_diff$residuals)
Pacf(model_diff$residuals)

preds_diff <- forecast(model_diff, 10)
autoplot(preds_diff) +
  xlim(1400, 1490)


p <- diff_data[6:nrow(diff_data), ] %>% 
        ggplot(aes(time, diff_pm)) +
          geom_line(color = "blue") +
          geom_line(aes(y = preds_diff$fitted), color = "red")
ggplotly(p, dynamicTicks = TRUE) %>% layout(hovermode = "x")

```







#---------------------------------------------------- COVID Analysis ----------------------------------------------------#

```{r plot covid data}

# Calculate the number of new cases per day and the rolling 7 day average for new cases
covid_cases <- covid_cases %>% group_by(County) %>% 
  mutate(new_cases = c(0, diff(Cases))) %>% group_by(County) %>% 
  mutate(avg_7day = round(rollmeanr(new_cases, 7, fill = NA))) %>% ungroup() %>% as_tibble()

covid_cases %>% filter(County == "Harris")


# Plotting the total number of cases and fatalities per day in Harris County
covid_cases %>% filter(County %in% c("Dallas", 'Harris'), Date > "2020-03-06") %>%  ggplot(aes(x = Date, y = Cases)) + 
  geom_line(aes(color = County)) +
  geom_line(aes(y = covid_fatal[covid_fatal$County == "Harris",]$Fatalities, color = "red")) +
  

covid_fatal %>% filter(County == "Harris") %>%  ggplot(aes(x = Date, y = Fatalities)) + geom_line()



# Plotting the number of new cases
p <- covid_cases %>% filter(County == "Harris") %>% 
  ggplot(aes(x = Date, y = new_cases)) + 
    geom_line() +
    geom_line(aes(y = avg_7day), color = "red") 
ggplotly(p, dynamicTicks = TRUE) %>% layout(hovermode = "x")





```


```{r look into counties in pm2.5 dataset}

counties <- unique(pm2.5_texas$county)

covid_cases %>% filter(County %in% counties) %>% select(County) %>% unique()

```



























































































































































































